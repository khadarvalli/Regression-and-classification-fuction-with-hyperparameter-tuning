{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder,LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.metrics import silhouette_score,cohen_kappa_score,explained_variance_score,jaccard_score,log_loss\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,confusion_matrix,roc_auc_score,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score,explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression,SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "sgb=SGDRegressor()\n",
    "dt=DecisionTreeRegressor()\n",
    "\n",
    "parameters={\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "            \"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11,12],\n",
    "           \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10],\n",
    "           \"min_weight_fraction_leaf\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "           \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
    "           \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90] }\n",
    "\n",
    "rf=RandomForestRegressor()\n",
    "param_grid = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "             'max_depth'’: [80, 90, 100, 110],\n",
    "             'max_features'’: [2, 3],\n",
    "             'min_samples_leaf'’: [3, 4, 5],\n",
    "             \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
    "             'min_samples_split'’: [8, 10, 12],\n",
    "             'n_estimators'’: [100, 200, 300, 1000]}\n",
    "ada=AdaBoostRegressor()\n",
    "param_grid={'n_estimators':[50,500,1000,2000],\n",
    "            'learning_rate':[.001,0.01,.1,1],\n",
    "           'loss' : ('linear', 'square', 'exponential')}\n",
    "gb=GradientBoostingRegressor()\n",
    "param_grid ={\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "             'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001],\n",
    "             'n_estimators':[100,250,500,750,1000,1250,1500,1750],\n",
    "             'subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "             'max_depth':[2,3,4,5,6,7],\n",
    "             'min_samples_split':[2,4,6,8,10,20,40,60,100],\n",
    "             'min_samples_leaf':[1,3,5,7,9],\n",
    "             'max_features':[2,3,4,5,6,7],\n",
    "             'max_features' : ('auto', 'sqrt', 'log2')\n",
    "             }\n",
    "xgb=XGBRegressor()\n",
    "param_grid = {\n",
    "             'max_depth':range(3,10,2),\n",
    "             'min_child_weight':range(1,6,2),\n",
    "             'gamma':[i/10.0 for i in range(0,5),\n",
    "             'subsample':[i/10.0 for i in range(6,10)],\n",
    "             'colsample_bytree':[i/10.0 for i in range(6,10),\n",
    "             'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "            'learning_rate' : [0.05,0.1]}\n",
    "\n",
    "svm=SVR()\n",
    "param_grid = {'C': [0.1,1, 10, 100], \n",
    "              'gamma': [1,0.1,0.01,0.001],\n",
    "              'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "knn=KNeighborsRegressor()\n",
    "param_grid = \n",
    "            {'n_neighbors': list(range(1,30),\n",
    "            'algorithm' :('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
    "            'leaf_size': (20,40,1),\n",
    "            'p': (1,2),\n",
    "            'weights': ('uniform', 'distance'),\n",
    "            'metric': ('minkowski', 'chebyshev')}\n",
    "                                \n",
    "\n",
    "lgbm=LGBMRegressor()\n",
    "param_grid = {'n_estimator':[100,200],\n",
    "              'num_leaves': [256,128],\n",
    "             'max_depth': [5, 8, 10],\n",
    "             'learning_rate': [0.05, 0.1]}                                \n",
    "cb=CatBoostRegressor()\n",
    "param_grid = { \"learning_rate\": np.linspace(0,0.2,5)\n",
    "             'iterations': [100, 150, 200],\n",
    "            'depth': [2, 4, 6, 8],\n",
    "            'l2_leaf_reg': [0.2, 0.5, 1, 3]}                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "param_grid = {\n",
    "            'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0],\n",
    "            'penalty': ['l2'],\n",
    "            'n_jobs': [-1],\n",
    "             'max_iter'=[100,300,500]}\n",
    "\n",
    "\n",
    "sgb=SGDClassifier()\n",
    "param_grid = {\n",
    "            'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n",
    "            'n_iter': [1000], # number of epochs\n",
    "            'loss': ['log'], # logistic regression,\n",
    "            'penalty': ['l2'],\n",
    "            'n_jobs': [-1]}\n",
    "\n",
    "dt=DecisionTreeClassifier()\n",
    "param_grid={\"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\":[\"best\",\"random\"],\n",
    "            \"max_depth\" : [1,3,5,7,9,11,12],\n",
    "            \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10],\n",
    "            \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
    "            \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90] }\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "param_grid = {\"criterion\" : ['gini', 'entropy'],\n",
    "             'max_depth'’: [80, 90, 100, 110],\n",
    "             'max_features'’: [2, 3],\n",
    "             'min_samples_leaf'’: [3, 4, 5],\n",
    "             \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
    "             'min_samples_split'’: [8, 10, 12],\n",
    "             'n_estimators'’: [100, 200, 300, 1000]}\n",
    "\n",
    "ada=AdaBoostClassifier()\n",
    "param_grid={'n_estimators':[500,1000,2000],\n",
    "             'learning_rate':[.001,0.01,.1]}\n",
    "\n",
    "gb=GradientBoostingClassifier()\n",
    "param_grid ={'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001],\n",
    "             'n_estimators':[100,250,500,750,1000,1250,1500,1750],\n",
    "             'max_depth':[2,3,4,5,6,7],\n",
    "             'min_samples_split':[2,4,6,8,10,20,40,60,100],\n",
    "             'min_samples_leaf':[1,3,5,7,9],\n",
    "             'max_features':[2,3,4,5,6,7],\n",
    "             'subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1]}\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "param_grid = {\n",
    "             'max_depth':range(3,10,2),\n",
    "             'min_child_weight':range(1,6,2),\n",
    "             'gamma':[i/10.0 for i in range(0,5),\n",
    "             'subsample':[i/10.0 for i in range(6,10)],\n",
    "             'colsample_bytree':[i/10.0 for i in range(6,10),\n",
    "             'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "            'learning_rate' : [0.05,0.1]}\n",
    "\n",
    "\n",
    "svm=SVC()\n",
    "param_grid = {'C': [0.1,1, 10, 100], \n",
    "              'gamma': [1,0.1,0.01,0.001],\n",
    "              'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "param_grid = {\n",
    "            'n_neighbors': list(range(1,30),\n",
    "            'algorithm' :('auto', 'ball_tree', 'kd_tree', 'brute')\n",
    "            'leaf_size': (20,40,1),\n",
    "            'p': (1,2),\n",
    "            'weights': ('uniform', 'distance'),\n",
    "            'metric': ('minkowski', 'chebyshev')}\n",
    "\n",
    "lgbm=LGBMClassifier()\n",
    "param_grid = {'n_estimator':[100,200],\n",
    "              'num_leaves': [256,128],\n",
    "             'max_depth': [5, 8, 10],\n",
    "             'learning_rate': [0.05, 0.1]}\n",
    "\n",
    "cb=CatBoostClassifier()\n",
    "param_grid = { \"learning_rate\": np.linspace(0,0.2,5)\n",
    "             'iterations': [100, 150, 200],\n",
    "            'learning_rate': [0.03, 0.1],\n",
    "            'depth': [2, 4, 6, 8],\n",
    "            'l2_leaf_reg': [0.2, 0.5, 1, 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(model, param_grid, refit = True, verbose = 3, n_jobs = -1) \n",
    "  \n",
    "# fit the model for grid search \n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 1000,10)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10,14]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,6,8]\n",
    "# Create the random grid\n",
    "parameters = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               \n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['friedman_mse','mse','mae']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
